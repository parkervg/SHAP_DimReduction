{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon-review-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6o1qlJREcEF",
        "outputId": "6ea31ae6-b0a3-482a-856c-8df460a9c02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"drive/My Drive/shap-dim-reduction/shap-dim-reduction\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/shap-dim-reduction/shap-dim-reduction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq80fJN9IoCN",
        "outputId": "0cc37b2e-b74c-43ba-c65a-89eb8a3423f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "  ! git config --global user.email \"EMAIL\"\n",
        "  ! git config --global user.name \"USERNAME\"\n",
        "  ! git pull\n",
        "except:\n",
        "  ! git stash \n",
        "  ! git pull"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects:   9% (1/11)\u001b[K\rremote: Counting objects:  18% (2/11)\u001b[K\rremote: Counting objects:  27% (3/11)\u001b[K\rremote: Counting objects:  36% (4/11)\u001b[K\rremote: Counting objects:  45% (5/11)\u001b[K\rremote: Counting objects:  54% (6/11)\u001b[K\rremote: Counting objects:  63% (7/11)\u001b[K\rremote: Counting objects:  72% (8/11)\u001b[K\rremote: Counting objects:  81% (9/11)\u001b[K\rremote: Counting objects:  90% (10/11)\u001b[K\rremote: Counting objects: 100% (11/11)\u001b[K\rremote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects:  20% (1/5)\u001b[K\rremote: Compressing objects:  40% (2/5)\u001b[K\rremote: Compressing objects:  60% (3/5)\u001b[K\rremote: Compressing objects:  80% (4/5)\u001b[K\rremote: Compressing objects: 100% (5/5)\u001b[K\rremote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 8 (delta 6), reused 5 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n",
            "From https://github.com/parkervg/shap-dim-reduction\n",
            "   27de20c..bbab4c9  master     -> origin/master\n",
            "Updating 27de20c..bbab4c9\n",
            "Fast-forward\n",
            " lib/ProcessEmbeddings.py | 8 \u001b[32m+++++++\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 7 insertions(+), 1 deletion(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "746QWXAJII4d"
      },
      "source": [
        "try:\n",
        "  import shap \n",
        "except:\n",
        "  !pip install shap"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWKX6_NgK0WB",
        "outputId": "d5daa4f3-7187-4126-ed80-c35ddbbd46f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUMh99GaEHrp"
      },
      "source": [
        "import bz2\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from lib.ProcessEmbeddings import WordEmbeddings\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pNEeGHLFbIz"
      },
      "source": [
        "def standard_glove():\n",
        "  WE = WordEmbeddings(vector_file=\"embeds/glove.6B.300d.txt\")\n",
        "  word_vectors = WE.get_vector_dict()\n",
        "  wvec_dim = WE.embeds.shape[1]\n",
        "  return word_vectors, wvec_dim"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1kXnhsjLmIV"
      },
      "source": [
        "def shap_glove(task):\n",
        "  WE = WordEmbeddings(vector_file=\"embeds/glove.6B.300d.txt\")\n",
        "  dims = WE.shap_dim_reduction(task=task, k=50)\n",
        "  word_vectors = WE.get_vector_dict()\n",
        "  wvec_dim = WE.embeds.shape[1]\n",
        "  return word_vectors, wvec_dim, dims"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3j6kfAzQssR"
      },
      "source": [
        "def rand_dims_glove():\n",
        "  WE = WordEmbeddings(vector_file=\"embeds/glove.6B.300d.txt\")\n",
        "  dims = WE.rand_dim_reduction(k=50)\n",
        "  word_vectors = WE.get_vector_dict()\n",
        "  wvec_dim = WE.embeds.shape[1]\n",
        "  return word_vectors, wvec_dim, dims"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki3d9s-oEPaQ"
      },
      "source": [
        "def labels_text(x):\n",
        "  label=[]\n",
        "  text=[]\n",
        "  for line in bz2.BZ2File(x):\n",
        "    decode = line.decode(\"utf-8\")\n",
        "    label.append(int(decode[9]) - 1)\n",
        "    text.append(decode[10:].strip())\n",
        "  return np.array(label),text\n",
        "\n",
        "train_label, train_text = labels_text('data/amazon-reviews/train.ft.txt.bz2')\n",
        "test_label, test_text = labels_text('data/amazon-reviews/test.ft.txt.bz2')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOHxoceFPC6M"
      },
      "source": [
        "We're only using a subsection of the dataset, so we shuffle it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV-dg63cPA67"
      },
      "source": [
        "train_text, train_label = shuffle(train_text, train_label)\n",
        "test_text, test_label = shuffle(test_text, test_label)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU9cMeXTHR8u"
      },
      "source": [
        "train_text = train_text[:10000]\n",
        "train_label = train_label[:10000]\n",
        "test_text = test_text[:5000]\n",
        "test_label = test_label[:5000]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwMyTPnvUVcU"
      },
      "source": [
        "Make sure there's no overlap in amazon-review data and CR data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCKmN6OeUfzC"
      },
      "source": [
        "# all_lines = []\n",
        "# with open(\"SentEval/data/downstream/CR/custrev.neg\", \"r\") as f:\n",
        "#   for line in f:\n",
        "#     all_lines.append(line.strip())\n",
        "# with open(\"SentEval/data/downstream/CR/custrev.pos\", \"r\") as f:\n",
        "#   for line in f:\n",
        "#     all_lines.append(line.strip())\n",
        "# for line1 in all_lines:\n",
        "#   for line2 in train_text + test_text:\n",
        "#     if re.sub(r'\\s+', '', line1.lower()) == re.sub(r'\\s+', '', line2.lower()):\n",
        "#       print(\"DUPLICATE\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BXrvhpuEoJV"
      },
      "source": [
        "def get_sent_vectors(sents, word_vectors, wvec_dim):\n",
        "  \"\"\"\n",
        "  Averages over all word vectors in a sentence.\n",
        "  \"\"\"\n",
        "  embeddings = []\n",
        "  for sent in sents:\n",
        "      sentvec = []\n",
        "      for word in word_tokenize(sent):\n",
        "        try:\n",
        "            sentvec.append(word_vectors[word.lower()])\n",
        "        except:\n",
        "            pass\n",
        "      if not sentvec:\n",
        "          vec = np.zeros(wvec_dim)\n",
        "          sentvec.append(vec)\n",
        "      sentvec = np.mean(sentvec, 0)\n",
        "      embeddings.append(sentvec)\n",
        "  embeddings = np.vstack(embeddings)\n",
        "  return embeddings"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAygqewVHTka",
        "outputId": "dbadb6b7-043c-4117-d8b8-109ead6e57cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_vectors, wvec_dim = standard_glove()\n",
        "Xtrain_glove = get_sent_vectors(train_text, word_vectors, wvec_dim)\n",
        "Xtest_glove = get_sent_vectors(test_text, word_vectors, wvec_dim)\n",
        "\n",
        "word_vectors, wvec_dim, cr_dims = shap_glove(\"CR\")\n",
        "Xtrain_shap_cr = get_sent_vectors(train_text, word_vectors, wvec_dim)\n",
        "Xtest_shap_cr = get_sent_vectors(test_text, word_vectors, wvec_dim)\n",
        "\n",
        "word_vectors, wvec_dim, trec_dims = shap_glove(\"TREC\")\n",
        "Xtrain_shap_trec = get_sent_vectors(train_text, word_vectors, wvec_dim)\n",
        "Xtest_shap_trec = get_sent_vectors(test_text, word_vectors, wvec_dim)\n",
        "\n",
        "word_vectors, wvec_dim, mr_dims = shap_glove(\"MR\")\n",
        "Xtrain_shap_mr = get_sent_vectors(train_text, word_vectors, wvec_dim)\n",
        "Xtest_shap_mr = get_sent_vectors(test_text, word_vectors, wvec_dim)\n",
        "\n",
        "word_vectors, wvec_dim, rand_dims = rand_dims_glove()\n",
        "Xtrain_rand = get_sent_vectors(train_text, word_vectors, wvec_dim)\n",
        "Xtest_rand = get_sent_vectors(test_text, word_vectors, wvec_dim)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 19:13:17 INFO     \u001b[36mLoading vectors at embeds/glove.6B.300d.txt...\u001b[0m\n",
            "2020-11-08 19:13:49 INFO     \u001b[36mLoading vectors at embeds/glove.6B.300d.txt...\u001b[0m\n",
            "2020-11-08 19:14:08 INFO     \u001b[37mFound 5282 words with word vectors, out of             5677 words\u001b[0m\n",
            "2020-11-08 19:14:08 INFO     Generating sentence embeddings\n",
            "2020-11-08 19:14:08 INFO     Generated sentence embeddings\n",
            "2020-11-08 19:14:08 INFO     Training sklearn-LogReg with (inner) 5-fold cross-validation\n",
            "2020-11-08 19:14:11 INFO     Best param found at split 1: l2reg = 2                 with score 78.01\n",
            "2020-11-08 19:14:13 INFO     Best param found at split 2: l2reg = 2                 with score 77.45\n",
            "2020-11-08 19:14:16 INFO     Best param found at split 3: l2reg = 4                 with score 78.71\n",
            "2020-11-08 19:14:18 INFO     Best param found at split 4: l2reg = 4                 with score 78.34\n",
            "2020-11-08 19:14:21 INFO     Best param found at split 5: l2reg = 1                 with score 77.65\n",
            "2020-11-08 19:14:21 INFO     \u001b[36mOriginal accuracy on task CR: 78.36\u001b[0m\n",
            "2020-11-08 19:14:21 INFO     \u001b[37mClassifier has 2 classes\u001b[0m\n",
            "2020-11-08 19:14:21 INFO     \u001b[36mNew shape of embeds is (400000, 50)\u001b[0m\n",
            "2020-11-08 19:14:33 INFO     \u001b[36mLoading vectors at embeds/glove.6B.300d.txt...\u001b[0m\n",
            "2020-11-08 19:14:52 INFO     ***** Transfer task : TREC *****\n",
            "\n",
            "\n",
            "2020-11-08 19:14:53 INFO     \u001b[37mFound 5867 words with word vectors, out of             9779 words\u001b[0m\n",
            "2020-11-08 19:14:53 INFO     Computed train embeddings\n",
            "2020-11-08 19:14:53 INFO     Computed test embeddings\n",
            "2020-11-08 19:14:53 INFO     Training sklearn-LogReg with 5-fold cross-validation\n",
            "2020-11-08 19:15:11 INFO     [('reg:0.5', 63.52), ('reg:1', 63.15), ('reg:2', 62.82), ('reg:4', 62.8), ('reg:8', 62.56), ('reg:16', 62.45), ('reg:32', 62.38)]\n",
            "2020-11-08 19:15:11 INFO     Cross-validation : best param found is reg = 0.5             with score 63.52\n",
            "2020-11-08 19:15:11 INFO     Evaluating...\n",
            "2020-11-08 19:15:12 INFO     \u001b[36mOriginal accuracy on task TREC: 67.0\u001b[0m\n",
            "2020-11-08 19:15:12 INFO     \u001b[37mClassifier has 6 classes\u001b[0m\n",
            "2020-11-08 19:15:12 INFO     \u001b[36mNew shape of embeds is (400000, 50)\u001b[0m\n",
            "2020-11-08 19:15:24 INFO     \u001b[36mLoading vectors at embeds/glove.6B.300d.txt...\u001b[0m\n",
            "2020-11-08 19:15:44 INFO     \u001b[37mFound 17690 words with word vectors, out of             21404 words\u001b[0m\n",
            "2020-11-08 19:15:44 INFO     Generating sentence embeddings\n",
            "2020-11-08 19:15:44 INFO     Generated sentence embeddings\n",
            "2020-11-08 19:15:44 INFO     Training sklearn-LogReg with (inner) 5-fold cross-validation\n",
            "2020-11-08 19:15:49 INFO     Best param found at split 1: l2reg = 0.25                 with score 75.85\n",
            "2020-11-08 19:15:55 INFO     Best param found at split 2: l2reg = 2                 with score 75.5\n",
            "2020-11-08 19:16:01 INFO     Best param found at split 3: l2reg = 4                 with score 75.91\n",
            "2020-11-08 19:16:06 INFO     Best param found at split 4: l2reg = 8                 with score 75.82\n",
            "2020-11-08 19:16:12 INFO     Best param found at split 5: l2reg = 1                 with score 75.65\n",
            "2020-11-08 19:16:12 INFO     \u001b[36mOriginal accuracy on task MR: 75.44\u001b[0m\n",
            "2020-11-08 19:16:13 INFO     \u001b[37mClassifier has 2 classes\u001b[0m\n",
            "2020-11-08 19:16:13 INFO     \u001b[36mNew shape of embeds is (400000, 50)\u001b[0m\n",
            "2020-11-08 19:16:25 INFO     \u001b[36mLoading vectors at embeds/glove.6B.300d.txt...\u001b[0m\n",
            "2020-11-08 19:16:44 INFO     \u001b[36mRandomly selected dimension indices [145, 248, 35, 242, 117, 255, 54, 280, 131, 281, 111, 108, 166, 27, 146, 110, 107, 147, 216, 5, 70, 7, 275, 175, 172, 258, 251, 187, 65, 222, 234, 67, 270, 238, 160, 69, 93, 184, 102, 261, 74, 21, 112, 83, 257, 60, 140, 114, 49, 31]\u001b[0m\n",
            "2020-11-08 19:16:44 INFO     \u001b[36mNew shape of embeds is (400000, 50)\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jjA5IKNZpwN"
      },
      "source": [
        "Full 300 dimension Glove accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AwzCMtFHf9f",
        "outputId": "3e557491-3997-4920-e245-3302643b542c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(Xtrain_glove, train_label)\n",
        "print(round(clf.score(Xtest_glove, test_label), 2))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-2l8fu7Pbpj",
        "outputId": "112550b2-9120-4dc1-c824-901009026d9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "WE = WordEmbeddings(vector_file=\"embeds/glove.6B.300d.txt\")\n",
        "glove_dims = WE.top_shap_dimensions(clf, np.vstack((Xtrain_glove, Xtest_glove)), k=50)\n",
        "Xtrain_glove_reduced = np.take(Xtrain_glove, indices=glove_dims, axis=1)\n",
        "Xtest_glove_reduced = np.take(Xtest_glove, indices=glove_dims, axis=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 19:18:00 INFO     \u001b[36mLoading vectors at embeds/glove.6B.300d.txt...\u001b[0m\n",
            "2020-11-08 19:18:20 INFO     \u001b[37mClassifier has 2 classes\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAkb5_9RQ9cp"
      },
      "source": [
        "SHAP amazon-review reduced 50 dimension Glove accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOzqC3UjRB8g",
        "outputId": "27d842c8-6b01-43bc-a316-6320ffd31df7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(Xtrain_glove_reduced, train_label)\n",
        "print(round(clf.score(Xtest_glove_reduced, test_label), 2))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhNRpTvCZsrR"
      },
      "source": [
        "SHAP CR reduced 50 dimension accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdJWA879LWKz",
        "outputId": "2b8b0e53-4748-4d63-d7dc-0e74a018845b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(Xtrain_shap_cr, train_label)\n",
        "print(round(clf.score(Xtest_shap_cr, test_label), 2))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGhFNq9wf8NS"
      },
      "source": [
        "SHAP reduced 50 dimensions on TREC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EkADnGqigo4",
        "outputId": "f95afe7d-90ee-4879-e0a4-4b9ea081196f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(Xtrain_shap_trec, train_label)\n",
        "print(round(clf.score(Xtest_shap_trec, test_label), 2))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fNW9JvM27z"
      },
      "source": [
        "SHAP reduced 50 dimensions on MR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAtOuzmJM5Na",
        "outputId": "5c88b36b-27ae-43f7-cd26-8b623ed7722c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(Xtrain_shap_mr, train_label)\n",
        "print(round(clf.score(Xtest_shap_mr, test_label), 2))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKRmzDoBZvay"
      },
      "source": [
        "Randomly reduced 50 dimension accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0wZIm8qTE-O",
        "outputId": "d4012800-dca4-4977-b679-6f25577e619f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(Xtrain_rand, train_label)\n",
        "print(round(clf.score(Xtest_rand, test_label), 2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c81EEDvmRkua",
        "outputId": "1f147a66-d885-4ee3-9b6f-fde374860c20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def intersect(a, b):\n",
        "  return len(list(set(a) & set(b)))\n",
        "print(f\"Intersecting dims between CR and amazon-review: {intersect(glove_dims, cr_dims)}\")\n",
        "print()\n",
        "print(f\"Intersecting dims between MR and amazon-review: {intersect(glove_dims, mr_dims)}\")\n",
        "print()\n",
        "print(f\"Intersecting dims between TREC and amazon-review: {intersect(glove_dims, trec_dims)}\")\n",
        "print()\n",
        "print(f\"Intersecting dims between random dims and amazon-review: {intersect(rand_dims, trec_dims)}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intersecting dims between CR and amazon-review: 19\n",
            "\n",
            "Intersecting dims between MR and amazon-review: 17\n",
            "\n",
            "Intersecting dims between TREC and amazon-review: 11\n",
            "\n",
            "Intersecting dims between random dims and amazon-review: 11\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
